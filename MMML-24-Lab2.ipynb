{"cells":[{"cell_type":"markdown","metadata":{"id":"uYSAaNS-61j6"},"source":["# MMML Lab Assignment 2: SVM and optimization\n","\n","The aim of this lab assignment is to implement the Support Vector Machines (SVM) classification algorithm. We will discuss \n","*   the classical separating hyperplane (with hard margins)\n","*   separating hyperplane (with soft margins)\n","*   nonlinear separation (using kernels)  \n","\n","and understand how the respective opitmization problems (including the dual Wolfe problem) can be solved numerically. Answering all questions will bring 7 points out of the total course grade.\n","\n","Good sources on SVM algorithms are \n","\n","*   [Support Vector Machines Succintly](https://www.syncfusion.com/succinctly-free-ebooks/support-vector-machines-succinctly) an introductory explanation of all related questions\n","*   [The Elements of Statistical Learning](https://hastie.su.domains/Papers/ESLII.pdf) an advanced treatment of the linear and non-linear SVM\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZOeHVyOGz-Em"},"source":["### Instructions \n","\n","1.   Form a team of two\n","2.   Rename the ipynb file to `Name1_Name2_MMML_Lab2.ipynb`\n","3.   Indicate team members at the top\n","4.   Change the number `blobs_random_seed` to the sum of your birthdays (i.e., day of the month)\n","5.   Provide your solutions (code or explanation) as necessary; do not reshuffle the cell order! You are welcome to optimize the existing commands if you want \n","6.   Please execute all the cells before submission; make sure there are no errors, all plots have been generated, and all numerical answers calculated. Also, do not make *looong* prints\n","7.   Submit your notebook to **cms** along with the pdf output.  \n"]},{"cell_type":"markdown","metadata":{"id":"M7Tv7PIyzYBF"},"source":["## 1. Preparation and Data Generation **(0.25 pt)**\n","\n","### 1.1 Import packages\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PkcvNTkE61j_"},"outputs":[],"source":["# Imports\n","from sklearn import svm\n","from sklearn.datasets import make_blobs\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import random\n","import warnings\n","from typing import List, Tuple\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# fix seed for reproducible results\n","np.random.seed(0)\n","random.seed(0)"]},{"cell_type":"markdown","metadata":{"id":"OFsnufXK61kA"},"source":["### 1.2 Fix configuration parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3LA3ZKlM61kB"},"outputs":[],"source":["# Configuration options\n","\n","# ========= YOUR CODE STARTS HERE ========= #\n","blobs_random_seed = 25\n","# ========== YOUR CODE ENDS HERE ========== #\n","\n","centers = [(0,0), (5,5)]\n","cluster_std = 1\n","frac_test_split = 0.33\n","num_features_for_samples = 1\n","num_samples_total = 1000"]},{"cell_type":"markdown","metadata":{"id":"d52C9C_E61kC"},"source":["### 1.3 Now we generate data, split for test/train datasets, and visualize the train dataset"]},{"cell_type":"markdown","metadata":{"id":"klbm4MkqukBQ"},"source":["The data are of the form $(\\mathbf{x}_j,y_j)$, $j = 1,\\dots, n$, where $\\mathbf{x}_j \\in \\mathbb{R}^2$, and $y_j=1$ for red points and $y_j=-1$ for blue ones. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fn77E89q61kC"},"outputs":[],"source":["# Generate data\n","def generate_data(num_samples_total: int, centers: List[Tuple[int, int]], num_features_for_samples: int, cluster_std: float) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n","    \n","    inputs, targets = make_blobs(n_samples = num_samples_total, centers = centers, n_features = num_features_for_samples, cluster_std = cluster_std)\n","    X_train, X_test, y_train, y_test = train_test_split(inputs, targets, test_size=frac_test_split, random_state=blobs_random_seed)\n","    y_train = 2*y_train -1\n","    y_test = 2*y_test-1\n","    return X_train, X_test, y_train, y_test\n","\n","X_train, X_test, y_train, y_test = generate_data(num_samples_total, centers, num_features_for_samples, cluster_std)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nx0jPyLADhe_"},"outputs":[],"source":["# separate blue and red points for convenience\n","def split_data(X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.float32]:\n","    red = X[np.where(y==1)]\n","    blue = X[np.where(y==-1)]\n","    # this is the direction vector to compare to \n","    p = np.mean(red, axis = 0) - np.mean(blue, axis = 0)\n","    return red, blue, p\n","\n","red, blue, p = split_data(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"executionInfo":{"elapsed":986,"status":"ok","timestamp":1679309851830,"user":{"displayName":"Rostyslav Hryniv","userId":"02208197071590517236"},"user_tz":-120},"id":"n4ikVi0C61kD","outputId":"1c444016-81f3-4124-db9d-4cd62d1a80de"},"outputs":[],"source":["# Generate scatter plots for train and test data \n","def plot_data(X_train: np.ndarray, X_test: np.ndarray, y_train: np.ndarray, y_test: np.ndarray):\n","    plt.figure(figsize=(10,5))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=10, cmap=plt.cm.Paired)\n","    plt.title(\"Train data\")\n","\n","    plt.subplot(1, 2, 2)\n","    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=10, cmap=plt.cm.Paired)\n","    plt.title(\"Test data\")\n","\n","    # plt.show()\n","\n","plot_data(X_train, X_test, y_train, y_test)"]},{"cell_type":"markdown","metadata":{"id":"-UJnQUlWTw_2"},"source":["### 1.4 Change the parameter `cluster_std` to 2 and regenerate the data.  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ========= YOUR CODE STARTS HERE ========= #\n","# cluster_std = 2\n","X_train, X_test, y_train, y_test = generate_data(num_samples_total, centers, num_features_for_samples, cluster_std)\n","plot_data(X_train, X_test, y_train, y_test)\n","# ========== YOUR CODE ENDS HERE ========== #"]},{"cell_type":"markdown","metadata":{"id":"NbwWAf1nsv60"},"source":["#### **Question 1.1 (0.25 pt)**  \n","\n","*   What do you think that parameter (`cluster_std`) is responsible for? What effect does it have on the data distribution?\n","---\n","Judging by the looks of the plotted data and the name of the variable, it would make sense that (`cluster_std`) would be responsible for the standard deviation within the clusters of points, i.e. in practice how likely are we to see them intersect. As we can see, for the regenerated data, they are no longer separable\n","\n","---\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JaSChrL_t_yQ"},"source":["### 1.5 We will use the values `cluster_std = 1` and  `cluster_std = 2` for the hard and soft margin classifiers\n","\n","**Remark**: Decrease 1 in `cluster_std = 1` if necessary to get separable sets; increase 2 in `cluster_std = 2` to get non-separable sets"]},{"cell_type":"markdown","metadata":{"id":"ANvH4RGGtkuj"},"source":["### 1.6 Visualization function\n","\n","This is the function to visualize the optimal hyperplane and the margin\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mIfeI4hft3g0"},"outputs":[],"source":["#  decision function\n","def decision_funct(x: np.ndarray, w: np.ndarray, b: np.float32) -> np.ndarray:\n","  return np.dot(x, w) + b   \n","\n","#  visualization\n","def visualSVM(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: np.float32, support: List[int]):\n","  plt.scatter(X[:, 0], X[:, 1], c=y, s=10, alpha = .5, cmap=plt.cm.Paired)\n","\n","  # plot the decision function\n","  ax = plt.gca()\n","  xlim = ax.get_xlim()\n","  ylim = ax.get_ylim()\n","\n","  # create grid to evaluate model\n","  xx = np.linspace(xlim[0], xlim[1], 30)\n","  yy = np.linspace(ylim[0], ylim[1], 30)\n","  YY, XX = np.meshgrid(yy, xx)\n","  xy = np.vstack([XX.ravel(), YY.ravel()]).T\n","  Z = decision_funct(xy, w, b).reshape(XX.shape)\n","\n","  # plot decision boundary and margins\n","  ax.contour(\n","    XX, YY, Z, colors=['r', 'b', 'r'], levels=[-1, 0, 1], alpha=0.5, linestyles=[\"--\", \"-\", \"--\"]\n","  )\n","  # plot support vectors\n","  ax.scatter(\n","    X[support, 0],\n","    X[support, 1],\n","    s=50,\n","    linewidth=1,\n","    facecolors=\"none\",\n","    edgecolors=\"g\",\n","  )"]},{"cell_type":"markdown","metadata":{"id":"QuMGKDNb1-kt"},"source":["We run the visualization with the normal vector $\\mathbf{w} = [1, -0.2]$,  free term $b = -3$, and the vectors $\\mathbf{x}_0$ and $\\mathbf{x}_{100}$ encircled. The hyperplane is not separating; try several other values to see the effect. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":266},"executionInfo":{"elapsed":781,"status":"ok","timestamp":1679309586505,"user":{"displayName":"Rostyslav Hryniv","userId":"02208197071590517236"},"user_tz":-120},"id":"YRQ_b5Zt2zA2","outputId":"a244d38d-e9fe-45ec-b3e4-2a7a5999391c"},"outputs":[],"source":["visualSVM(X_train, y_train, [1,-.2], -3, [0,100])"]},{"cell_type":"markdown","metadata":{"id":"6ZBcWMSKHwc0"},"source":["### 2.1: Choosing the optimal $b$ (0.5 pts)\n","\n","For a given normal $\\mathbf{w}\\in\\mathbb{R}^2$, find \n","*   the optimal position of the separating hyperplane $H : \\mathbf{x}\\cdot\\mathbf{w} + b = 0$\n","*   calculate the margin (or width) between classes. Margin is defined as distance between hyperplane and the observation (from each class), which is the closest to the hyperplane\n","*   rescale $\\mathbf{w}$ and $b$ properly\n","*   visualize the separating hyperplane and the support vectors\n","\n","The optimal separating hyperplane $$H:\\quad \\mathbf{x}\\cdot\\mathbf{w} + b = 0$$ with a fixed $\\mathbf{w}$ is determined by $b = -(b_+ + b_-)/2$, where\n","$$b_+ = \\min_{j:y_j = 1} \\, (\\mathbf{x}_j\\cdot\\mathbf{w}) \\qquad \\text{and} \\qquad b_- = \\max_{j:y_j = -1}\\, (\\mathbf{x}_j\\cdot\\mathbf{w})$$\n","and the margin (half-)width is \n","$$ d = \\text{(missing formula)} $$\n","The red points satisfying $\\mathbf{x}_j\\cdot\\mathbf{w} = b_+$ and the blue points satisfying $\\mathbf{x}_j\\cdot\\mathbf{w} = b_-$ are called __support vectors__ .To rephrase the formulas, these are points (from each class), which are the closest one to the separating hyperplane. \n","\n","**Remarks:**\n","*   Observe that the direction of the normal vector $\\mathbf{w}$ should be chosen so that $\\mathbf{p}\\cdot \\mathbf{w}>0$, where $\\mathbf{p}$ is the vector connecting the centroids of the blue ($y_j = -1$) and red ($y_j = 1$) points; this guarantees that $b_+ > b_-$ for _optimal separating_ hyperplanes. However, the inequality $b_+ > b_-$ is not guaranteed even if $\\mathbf{p}\\cdot \\mathbf{w}>0$; you should take this into account during scaling\n","*   After calculating $d$ for properly oriented $\\mathbf{w}$, rescale $\\mathbf{w}$ and $b$ so that $f(\\mathbf{x}) = \\pm1$ for support vectors\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ========= YOUR CODE STARTS HERE ========= #\n","visualSVM(X_train, y_train, [1,0.85], -4.8, [0,100])\n","# ========== YOUR CODE ENDS HERE ========== #"]},{"cell_type":"markdown","metadata":{"id":"vxqdhWQq61kE"},"source":["\n","---\n","## 2. First task: Hard margin SVM classifier **(2.5 pts)** ##  \n","\n","#### Given are the data $(\\mathbf{x}_j,y_j)_{j=1}^n$ with $\\mathbf{x}_j \\in \\mathbb{R}^2$ and $y_j = \\pm 1$. The points $\\mathbf{x}_j$ with $y_j=1$ are marked red and those with $y_j = -1$ blue. Under the assumption that red and blue points are linearly separable, the aim is to find the best ***linear classifier*** $f(\\mathbf{x}) = \\mathbf{x}\\cdot\\mathbf{w} + b$ such that $f(\\mathbf{x})>0$ for red points and $f(\\mathbf{x})<0$ for blue ones. We will determine $f$ using the training dataset and then evaluate its performance on the test dataset. \n","---\n","\n","\n","\n","\n","    "]},{"cell_type":"markdown","metadata":{"id":"cuA202ODj51D"},"source":["\n","#### **Question 2.1 (0.25 pt)**  \n","\n","*   Write down the formula for $d$\n","---\n","$$ d = \\frac{2}{\\|\\mathbf{w}\\|}$$\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"EiGdmGZ4Fr7Q"},"source":["---\n","#### **Question 2.2 (0.25 pt)**  \n","\n","*   Implement the function below:\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dIQ7schmADw"},"outputs":[],"source":["def optimal_b(X_train: np.ndarray, y_train: np.ndarray, w: np.ndarray, epsilon: np.float32 = 1e-3) -> Tuple[np.float32, np.ndarray, np.float32, np.ndarray]:\n","  # ========= YOUR CODE STARTS HERE ========= #\n","  # get red/blue labels\n","  red, blue, p = split_data(X_train, y_train)\n","\n","  # check if w is properly oriented \n","  w = w if np.dot(p, w) > 0 else -w\n","      \n","  # calculate b_red and the corresponding support vectors\n","  b_red = np.min(np.dot(red, w))\n","  # these are indices of the red support vectors; we allowed tolerance epsilon\n","  sprt_red = np.where((np.dot(X_train, w) <= b_red + epsilon) & (y_train == 1) )\n","  \n","  # calculate b_blue and the corresponding support vectors \n","  b_blue = np.max(np.dot(blue, w))\n","  sprt_blue = np.where((np.dot(X_train, w) >= b_blue - epsilon) & (y_train == -1) )\n","  # print(sprt_red, sprt_blue)\n","  \n","  # concatenate the support lists \n","  sprt = np.hstack((sprt_red, sprt_blue))\n","\n","  # calculate the actual width of the separating slab\n","  b = -(b_red + b_blue) / 2\n","  d = min(b + b_red, b_blue - b)\n","\n","  # the proper scaling factor for w and b\n","  sc_fact = d\n","  w = w / sc_fact   # rescale w\n","  b = b / sc_fact  # rescale b\n","\n","  # ========== YOUR CODE ENDS HERE ========== #\n","  return d, w, b, sprt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"elapsed":1495,"status":"ok","timestamp":1679309866289,"user":{"displayName":"Rostyslav Hryniv","userId":"02208197071590517236"},"user_tz":-120},"id":"Kuh19_z5ij0t","outputId":"d79142f0-a4fd-4db4-952a-e78a0fcc2a7c"},"outputs":[],"source":["### Test the performance of the function by choosing w = p\n","_, w, b, sprt = optimal_b(X_train, y_train, w=p)\n","print(sprt)\n","visualSVM(X_train, y_train, w, b, sprt)"]},{"cell_type":"markdown","metadata":{"id":"kkoFv7srUFR6"},"source":["### 2.2: The Perceptron Learning Algorithm (PLA) **(0.75 pts)**\n","\n","PLA is an iterative algorithm that updates the direction vector $\\mathbf{w}$ towards a misclassified example, one at a time. \n","\n","To explain the idea, let's first augment vectors $\\mathbf{x} = (x_1,x_2)^\\top \\in \\mathbb{R}^2$ to $\\hat{\\mathbf{x}} = (1,x_1,x_2)^\\top \\in \\mathbb{R}^3$ and $\\mathbf{w} = (w_1,w_2)^\\top$ to $\\hat{\\mathbf{w}} = (b ,w_1, w_2)^\\top$; then the hyperplane  $H: \\mathbf{w} \\cdot \\mathbf{x} + b = 0$ can be written as $\\hat{\\mathbf{w}}\\cdot \\hat{\\mathbf{x}} = 0$. Therefore, correctly classified vectors $\\mathbf{x}_j$ must satisfy the inequality \n","$$\n","  y_j (\\hat{\\mathbf{w}}\\cdot \\hat{\\mathbf{x}}_j) > 0.\n","$$\n","If $\\mathbf{x}_j$ is a ''red'' point (with $y_j = 1$) that is misclassified, then the angle between $\\hat{\\mathbf{w}}$ and $\\hat{\\mathbf{x}}_j$ is obtuse. The idea is that we should decrease the angle between them by updating $\\hat{\\mathbf{w}}$ to $\\hat{\\mathbf{w}} + \\hat{\\mathbf{x}}_j$ (see Fig. 1). Likewise, if $\\mathbf{x}_j$ is a misclassified ''blue'' point (with $y_j = -1$), then the angle between $\\hat{\\mathbf{w}}$ and $\\hat{\\mathbf{x}}_j$ is acute, and we increase it be replacing $\\hat{\\mathbf{w}}$ with $\\hat{\\mathbf{w}} - \\hat{\\mathbf{x}}_j$. \n","\n","<!DOCTYPE html>\n","<html lang=\"en\">\n","<head>\n","    <meta charset=\"UTF-8\">\n","    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n","    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n","    <title></title>\n","</head>\n","<body>\n","    <img src=\"https://drive.google.com/uc?export=view&id=12rduejeedS8NxrxXkSBJkkcDH3lB0k-R\">\n","\n","</body>\n","</html>\n","\n","This suggests the following classical **PLA**:\n","1.   Start with $\\hat{\\mathbf{w}}$ and classify the points\n","2.   Randomly select one of the misclassified points\n","3.   Update the $\\hat{\\mathbf{w}}$ and update classification\n","4.   Repeat 2 and 3 until there are misclassified points. \n","\n","It is proved that the PLA stops in a final number of steps if the data are linearly separable.\n","\n","For large datasets, forming a list of misclassified points is a costly operation, so one makes such a list, iterates through it, updates $\\hat{\\mathbf{w}}$ whenever needed, and only then forms the list of miscallified points afresh. \n"]},{"cell_type":"markdown","metadata":{"id":"2k-lwjwZIW2U"},"source":["#### **Question 2.3 (0.25 pt): Analyze the PLA update step** \n","Prove that by updating $\\hat{\\mathbf{w}}$, we are decreasing or increasing (as required) the angle between $\\hat{\\mathbf{w}}$ and $\\hat{\\mathbf{x}}_j$.\n","\n","---\n","Let's say that for an arbitrary point $\\mathbf{x_j}$, the dot product gives us an obtuse angle:\n","$$ \n","p = \\hat{\\mathbf{w}}\\cdot \\hat{\\mathbf{x}}_j = b + x_{1j}w_1 + x_{2j}w_2 \\lt 0\n","$$\n","If we update $\\hat{\\mathbf{w}}$ by adding $\\hat{\\mathbf{x}}_j$, then the dot product will increase:\n","$$\n","p' = \\hat{\\mathbf{w'}}\\cdot \\hat{\\mathbf{x}}_j = b + x_{1j}(w_1 + x_{1j}) + x_{2j}(w_2 + x_{2j}) = p + x_{1j}^2 + x_{2j}^2 \\ge p\n","$$\n","Giving us an angle closer to an acute one. The inverse is true if our angle is acute to begin with, and we update $\\hat{\\mathbf{w}}$ by subtracting $\\hat{\\mathbf{x}}_j$.\n","\n","Thus, to make an angle between a misclassified point and the normal vector closer to what we expect it to be, we update the normal vector by adding or subtracting the misclassified point, depending on which class it was supposed to be."]},{"cell_type":"markdown","metadata":{"id":"NYctyjxYHaO0"},"source":["---\n","#### **Question 2.4 (0.25 pt)**  \n","\n","*   Implement the functions below\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BtG0mqNKVR3V"},"outputs":[],"source":["#  The PLA algorithm\n","def misclassified(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: np.ndarray) -> np.ndarray:\n","  # ========= YOUR CODE STARTS HERE ========= #\n","  misclass = np.where(y * (np.dot(X, w) + b) <= 0)\n","  # ========== YOUR CODE ENDS HERE ========== #\n","  return misclass\n","\n","\n","def PLA(X_train: np.ndarray, y_train: np.ndarray, num_step: int = 1000) -> Tuple[np.float32, np.ndarray, np.float32]:\n","  # ========= YOUR CODE STARTS HERE ========= #\n","  # Initiate a hyperplane (45 degrees)\n","  w_initial = -np.array([1., 0.])\n","  b_initial = 0.\n","  _, w_initial, b_initial, _ = optimal_b(X_train, y_train, w_initial)\n","\n","  # find initial misclassified points\n","  misclass =  misclassified(X_train, y_train, w_initial, b_initial)[0]\n","  # print(misclass)\n","  print(\"Initial no. of misclassified points:\", len(misclass))\n","  \n","  # implement training loop\n","  step = 0\n","  b = b_initial\n","  w = w_initial\n","  while len(misclass)!=0 and step < num_step:\n","    step += 1\n","    # update the normal w of the hyperplane and b: \n","    rand_point = random.choice(misclass)\n","    # print(rand_point)\n","    if y_train[rand_point] == 1:\n","      w += X_train[rand_point]\n","      b += 1\n","    else:\n","      w -= X_train[rand_point]\n","      b -= 1\n","\n","    _, w, b, _ = optimal_b(X_train, y_train, w)\n","    # recalculate misclassified set\n","    misclass = misclassified(X_train, y_train, w, b)[0]\n","  \n","  # calculate final optimal parameters and margin\n","  d, w, b, sprt = optimal_b(X_train, y_train, w)\n","  misclass = misclassified(X_train, y_train, w, b)\n","  # ========== YOUR CODE ENDS HERE ========== #\n","  \n","  print(\"Number of steps required:\", step)\n","  print(\"The margin is:\", d, \"\\tw=\", w, \"\\tb=\", b)\n","  print(\"Final no. of misclassified points:\", len(misclass))\n","\n","  # calculate initial parameters for visualization\n","  _, w_initial, b_initial, _ = optimal_b(X_train, y_train, w_initial)\n","  misclass_initial =  misclassified(X_train, y_train, w_initial, b_initial)\n","  \n","  plt.figure(figsize=(10,5))\n","\n","  plt.subplot(1, 2, 1)\n","  visualSVM(X_train, y_train, w_initial, b_initial, misclass_initial) \n","  plt.title(\"Initial hyperplane\")\n","  \n","\n","  plt.subplot(1, 2, 2)\n","  visualSVM(X_train, y_train, w, b, sprt) \n","  plt.title(\"final separating hyperplane\")\n","  \n","  plt.show()\n","  return (d, w, b)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"elapsed":1022,"status":"ok","timestamp":1679310616041,"user":{"displayName":"Rostyslav Hryniv","userId":"02208197071590517236"},"user_tz":-120},"id":"RZMZbfvFsM-j","outputId":"e66d1925-5fc0-4be8-fc66-97c9ea8bc709"},"outputs":[],"source":["d, w_pla, b_pla = PLA(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"wQWgwUbiyo0P"},"source":["#### Evaluate the accuracy of PLA:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v75MkEN7zb5d"},"outputs":[],"source":["# ========= YOUR CODE STARTS HERE ========= #\n","acc = (np.sum(y_test * decision_funct(X_test, w_pla, b_pla) > 0) / y_test.shape)[0]\n","print(\"The accuracy of PLA is\", acc)\n","# ========== YOUR CODE ENDS HERE ========== #"]},{"cell_type":"markdown","metadata":{"id":"FsuRJty8U-LF"},"source":["\n","#### **Question 2.5 (0.25pt): Discuss pros and cons of the PLA**\n","\n","PLA is a good method for perfectly separable data, however for non-separable data, it will not converge. Also, it does not optimize the margin at all, just ending up with any separating hyperplane from a set of possible ones, thus finding a valid, though suboptimal solution.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Td_jZqf46ipM"},"source":["### 2.3: Direct maximization of the margin **(0.25 pt)**\n","\n","Maximize the margin over the directions of $\\mathbf{w} = (\\cos\\phi, \\sin\\phi)^\\top$, find the best separating hyperplane and evaluate its performance on the test dataset\n","\n","\n","Run through all directions $\\phi \\in [0,\\pi)$ and \n","*   find the optimal direction of the separating hyperplane\n","*   the optimal position (parameter $b$) of the separating hyperplane $H : \\mathbf{x}\\cdot\\mathbf{w} + b = 0$\n","*   visualize the separating hyperplane and the support vectors\n","*   calculate the margin (width)\n"]},{"cell_type":"markdown","metadata":{"id":"AN8eqRiffp68"},"source":["---\n","#### **Question 2.6 (0.25 pt)**  \n","\n","*   Implement the function below\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1344,"status":"ok","timestamp":1679310662453,"user":{"displayName":"Rostyslav Hryniv","userId":"02208197071590517236"},"user_tz":-120},"id":"hOR0Os0Jphcs","outputId":"ad56c137-9e39-4425-8e17-8b06dd82e44d"},"outputs":[],"source":["def direct_optimization(X_train: np.ndarray, y_train: np.ndarray, n: int = 1000) -> Tuple[np.float32, np.ndarray, np.float32]:\n","  # ========= YOUR CODE STARTS HERE ========= #\n","  max_d = float(\"-inf\")\n","  for i in range(n):\n","    phi = i*np.pi/n \n","    w = np.array([np.cos(phi), np.sin(phi)])\n","    d, w, b, sprt = optimal_b(X_train, y_train, w)\n","    # compare the current d with current maximum and update if necessary; save the optimal angle in phi_opt\n","    if d > max_d:\n","      max_d = d\n","      phi_opt = phi\n","\n","  # calculate final optimal parameters and margin\n","  w_opt = np.array([np.cos(phi_opt), np.sin(phi_opt)])\n","  d_opt, w_opt, b_opt, sprt = optimal_b(X_train, y_train, w_opt)\n","  # ========== YOUR CODE ENDS HERE ========== #\n","\n","  # illustrate decision for train and test data \n","  plt.figure(figsize=(10,5))\n","\n","  plt.subplot(1, 2, 1)\n","  sprt = np.where(y_train * decision_funct(X_train, w_opt, b_opt) < 1.01)[0]\n","  visualSVM(X_train, y_train, w_opt, b_opt, sprt)\n","  plt.title(\"Train data\")\n","\n","  plt.subplot(1, 2, 2)\n","  sprt = np.where(y_test * decision_funct(X_test, w_opt, b_opt) < 1.01)[0]\n","  visualSVM(X_test, y_test, w_opt, b_opt, sprt)\n","  plt.title(\"Test data\")\n","\n","  plt.show()\n","\n","  return (d_opt, w_opt, b_opt)\n","  \n","\n","d_opt, w_opt, b_opt = direct_optimization(X_train, y_train)\n","print(\"The margin is:\", d_opt, \"\\toptimal w is:\", w_opt, \"\\toptimal b is:\", b_opt)"]},{"cell_type":"markdown","metadata":{"id":"jXXdK9vq7WYF"},"source":["#### Evaluate the optimal classifier on the test set:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":296,"status":"ok","timestamp":1679262918487,"user":{"displayName":"Rostyslav Hryniv","userId":"02208197071590517236"},"user_tz":-120},"id":"XsMNGTTQaAZT","outputId":"ea1583d6-131d-409d-b4ad-17ba82f91759"},"outputs":[],"source":["# ========= YOUR CODE STARTS HERE ========= #\n","acc = (np.sum(y_test * decision_funct(X_test, w_opt, b_opt) > 0) / y_test.shape)[0]\n","print(\"The accuracy of direct optimization is\", acc)\n","# ========== YOUR CODE ENDS HERE ========== #"]},{"cell_type":"markdown","metadata":{"id":"TKmqD9XW61kF"},"source":["### 2.4 SVM as an optimization problem and dual Wolfe problem **(1 pts)**\n"]},{"cell_type":"markdown","metadata":{"id":"M8uS5T6F6ghw"},"source":["#### 2.4.1 Optimization problem\n","Let $f(\\mathbf{x}) = \\mathbf{x}\\cdot \\mathbf{w} + b$ be a linear classifier predicting the response variable $y$ via\n","$$\n","  y = \\operatorname{sign}(f(\\mathbf{x}))\n","$$\n","\n","The task is to maximize the margin, i.e., the value $M$ s.t., for all $j$,\n","$$\n","   y_j (\\mathbf{x}_j \\cdot \\mathbf{w} + b) \\ge M \\|\\mathbf{w}\\|\n","$$\n","By rescaling $\\mathbf{w}$ (and $b$) so that $M\\cdot\\|\\mathbf{w}\\| = 1$, we get an equivalent problem \n","\\begin{align*}\n","\\operatorname{argmin}_{\\mathbf{w}, b}  &\\frac12 \\|\\mathbf{w}\\|^2 \\\\\n","\\text{subject to } \\qquad &y_j (\\mathbf{x}_j \\cdot \\mathbf{w} + b) \\ge 1\n","\\end{align*}\n","\n","The Lagrange function is \n","$$\n","  \\mathscr{L}(\\mathbf{w}, b, \\lambda) = \\frac12\\|\\mathbf{w}\\|^2 + \\sum_j \\lambda_j  [1 - y_j(\\mathbf{x}_j\\cdot \\mathbf{w} + b)]\n","$$\n","Partial derivatives in $\\mathbf{w}$ and $b$ give the equalities \n","$$\n","    \\mathbf{w} = \\sum_j \\lambda_j y_j \\mathbf{x}_j \\qquad \\text{and} \\qquad 0 = \\sum_j \\lambda_j y_j; \\tag{1}\n","$$\n","and the slackness conditions (3) below for every $j$.\n","\n","We now use these expressions in $\\mathscr{L}$ to get the Wolfe dual problem of maximizing the quadratic problem\n","\\begin{equation}\n","   \\mathscr{L}_D = \\sum_j \\lambda_j - \\frac12 \\sum_{j,k} \\lambda_j\\lambda_k y_jy_k \\, \\mathbf{x}_j\\cdot\\mathbf{x}_k \\tag{2}\n","\\end{equation}\n","subject to the constraints $\\lambda_j \\ge 0$ and the slackness conditions that should be satisfied for every $j$:  \n","$$\n","   \\lambda_j [1 - y_j(\\mathbf{x}_j\\cdot \\mathbf{w} + b)] = 0 \\tag{3}\n","$$\n","Recall that the indices $j$ for which $y_j(\\mathbf{x}_j\\cdot \\mathbf{w} + b) = 1$ correspond to *support vectors* (i.e., points on the margin boundary). "]},{"cell_type":"markdown","metadata":{"id":"wPFlAxD2FbdL"},"source":["#### 2.4.2 Sequential Minimal Optimization\n","The Wolfe dual problem can be solved by applying the so called Sequential Minimal Optimization (SMO) method. The idea is to update only two (randomly chosen) Lagrange multipliers a time, say $\\lambda_i$ and $\\lambda_j$, in such a way that the constraint condition $\\sum_k \\lambda_ky_k = 0$ is preserved, the updated lambda's are non-negative, and the value of the function increases. \n","\n","To this end, we calculate the partial derivatives of $\\mathscr{L}$ in $\\lambda_i$ and $\\lambda_j$, form the partial gradient $\\mathbf{g}_{i,j}$, and move along the constraint line $\\ell = \\lambda_i y_i + \\lambda_j y_j = \\mathrm{const}$ to increase the value of $\\mathscr{L}_D$. If the direction vector $\\mathbf{d}:=(y_j, -y_i)$ of $\\ell$ forms an acute angle with $\\mathbf{g}_{i,j}$, we update the two lambda's in that direction; otherwise, in the opposite. The simplest implementation uses a constant learning rate."]},{"cell_type":"markdown","metadata":{"id":"aiPOBEbCgjL0"},"source":["--- \n","#### **Question 2.7 (0.75 pts)**  \n","\n","*   Implement the functions below\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tIFWv8yAFTdE"},"outputs":[],"source":["# calculate the partial derivative in lambda_j and fill in the missing line\n","def partial_derivative(X: np.ndarray, y:np.ndarray, lmbda: np.ndarray, j: int) -> np.float32:\n","  # ========= YOUR CODE STARTS HERE ========= #\n","  grad = 1 - np.sum(lmbda * y[j] * y * np.dot(X, X[j]))\n","  # ========== YOUR CODE ENDS HERE ========== #\n","  return grad"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":101309,"status":"ok","timestamp":1679310862805,"user":{"displayName":"Rostyslav Hryniv","userId":"02208197071590517236"},"user_tz":-120},"id":"0rOptZyCGnPw","outputId":"6c8a30bf-6dfc-48bc-93ed-851152990e2c"},"outputs":[],"source":["# complete the update rule for lambda_i and lambda_j\n","def dualSVM_SMO(X: np.ndarray, y: np.ndarray, lr: float = 1e-3, n_steps: int = 100000) -> np.ndarray:\n","  lmbda = np.zeros(X.shape[0])\n","  for k in range(n_steps):\n","    # ========= YOUR CODE STARTS HERE ========= #\n","    i, j = random.randint(0, X.shape[0]-1), random.randint(0, X.shape[0]-1)\n","    while y[i] == y[j]:\n","      j = random.randint(0, X.shape[0]-1)\n","\n","    grad_i = partial_derivative(X, y, lmbda, i)\n","    grad_j = partial_derivative(X, y, lmbda, j)\n","    \n","    line_grad = np.fromiter([y[j], -y[i]], dtype=np.float64)\n","    part_grad = np.fromiter([grad_i, grad_j], dtype=np.float64)\n","    proj = np.dot(line_grad, part_grad) / np.linalg.norm(line_grad) ** 2 * line_grad\n","\n","    if np.dot(part_grad, line_grad) < 0:\n","      proj = -proj\n","\n","    lmbda_i = lmbda[i]\n","    lmbda_j = lmbda[j]\n","    lmbda[i] += lr * proj[0]\n","    lmbda[j] += lr * proj[1]\n","\n","    if lmbda[i] < 0 or lmbda[j] < 0 or np.abs(np.dot(lmbda, y)) > 1e-3:\n","      lmbda[i] = lmbda_i\n","      lmbda[j] = lmbda_j\n","      print(\"KKT constraint failed\")\n","    else:\n","      print(lmbda[i], lmbda[j])\n","    # ========== YOUR CODE ENDS HERE ========== #\n","\n","  print(\"no. of steps:\", k, \"no of positive lambdas:\", len(lmbda[lmbda > 0]))\n","  assert len(lmbda[lmbda >= 0]) == len(lmbda)\n","  return(lmbda)\n","\n","lmbda = dualSVM_SMO(X_train,y_train,lr=1e-3, n_steps=10000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6o6y8S8yWciq"},"outputs":[],"source":["#  calculate the direction vector w from equation (1) in part 2.4.1.\n","# ========= YOUR CODE STARTS HERE ========= #\n","w = lmbda*y_train@X_train\n","d, w_smo, b_smo, sprt = optimal_b(X_train, y_train, w)\n","print(w)\n","print(w_smo, b_smo)\n","# ========== YOUR CODE ENDS HERE ========== #"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"executionInfo":{"elapsed":1375,"status":"ok","timestamp":1679310889215,"user":{"displayName":"Rostyslav Hryniv","userId":"02208197071590517236"},"user_tz":-120},"id":"f1JH-7NU0Ul0","outputId":"8c4f3c79-63f1-42b5-da67-fb40251a14db"},"outputs":[],"source":["# illustrate decision for train and test data \n","plt.figure(figsize=(10,5))\n","\n","plt.subplot(1, 2, 1)\n","idx = np.where(y_train * decision_funct(X_train, w_smo, b_smo) < 0)[0]\n","visualSVM(X_train, y_train, w_smo, b_smo, sprt)\n","plt.title(\"Train data\")\n","\n","plt.subplot(1, 2, 2)\n","idx = np.where(y_test * decision_funct(X_test, w_smo, b_smo) < 0)[0]\n","visualSVM(X_test, y_test, w_smo, b_smo, idx)\n","plt.title(\"Test data\")\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Calculate accuracy of the given Dual SVM SMO method"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":759,"status":"ok","timestamp":1679263076559,"user":{"displayName":"Rostyslav Hryniv","userId":"02208197071590517236"},"user_tz":-120},"id":"QIXmvYXnVfx1","outputId":"105884b0-ea29-48d2-959d-055f366902dc"},"outputs":[],"source":["# ========= YOUR CODE STARTS HERE ========= #\n","acc = (np.sum(y_test * decision_funct(X_test, w_smo, b_smo) > 0) / y_test.shape)[0]\n","print(\"The accuracy of dual SVM SMO is\", acc)\n","# ========== YOUR CODE ENDS HERE ========== #"]},{"cell_type":"markdown","metadata":{"id":"3BGNbKs6qefq"},"source":["#### **Question 2.8 (0.25 pt): Wolfe duality summary**\n","\n","Summarize in a few sequences how the dual Wolfe problem is used to find the optimal separation hyperplane and how it is solved using the SMO method\n","\n","---\n","> **Your explanations here**\n","---\n"]},{"cell_type":"markdown","metadata":{},"source":["#### **Question 2.9 (up to 1.5 pt): Advanced SMO (Additional!)**\n","\n","Think of some possible modifications to the given SMO algorithm (heuristics/another optimization approach). Provide detailed explanation for them, implement them and compare with the original SMO algorithm in terms of accuracy/efficiency"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ========= YOUR CODE STARTS HERE ========= #\n","\n","# ========== YOUR CODE ENDS HERE ========== #"]},{"cell_type":"markdown","metadata":{"id":"FCGLEPx78rXU"},"source":["## 3. Second task: Soft margin SVM classifier **(2 pts)**"]},{"cell_type":"markdown","metadata":{"id":"JQJRP7A1GIPM"},"source":["For the purpose of this last part, we'll recreate overlapping red and blue blobs with `cluster_std = 2`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DrlYinn2G38O"},"outputs":[],"source":["# generate data with cluster_std = 2\n","# ========= YOUR CODE STARTS HERE ========= #\n","X_train, X_test, y_train, y_test = ...\n","red, blue, p = ...\n","# ========== YOUR CODE ENDS HERE ========== #\n","plot_data(X_train, X_test, y_train, y_test)"]},{"cell_type":"markdown","metadata":{"id":"ekeWJVI685wk"},"source":["### 3.1 Slack variables and the objective function\n","\n","In most situations, the classes are not linearly separable, i.e., there is no hyperplane separating red and blue points in $\\mathbb{R}^n$ (cf. the simulated dataset with `cluster_std = 2`). In that case, we must accept that some training points will not be properly classified but try to keep their number low. Also, it is desirable to have a ''separating'' hyperplane with a relatively large margin ensuring robustness to noise. Thus our aim is to find a hyperplane $H$ and a margin $M = 1/\\|\\mathbf{w}\\|$ such that almost all training observations are properly classified and are outside the slab of width $M$ aroung the hyperplane $H$ and the misclassification penalty is low.\n","\n","To this end, we introduce the slack variables $\\xi_j\\ge0$ and impose relaxed  constraints\n","$$\n","  y_j[\\mathbf{x}_j\\cdot \\mathbf{w} + b] \\ge 1 - \\xi_j. \\tag{4}\n","$$\n","The slack variables\n","$$\n","  \\xi_j = \\max\\{0, 1- y_j[\\mathbf{x}_j \\cdot \\mathbf{w} + b]\\}\n","$$\n","measure incorrectness of classification: $\\xi_j = 0$ for correctly classified points outside the slab and $\\xi_j >0$ for points that fall inside the slab or are in the wrong halfspace. The new objective function is\n","$$\n","  f(\\mathbf{w},b,\\xi_j) = \\frac12\\|\\mathbf{w}\\|^2 + C \\sum_j \\xi_j\n","$$\n","under the constraints (4); here $C$ is the cost constant penalizing misclassification. \n"]},{"cell_type":"markdown","metadata":{"id":"wlaP4MteFZZ3"},"source":["#### **Question 3.1 (0.25 pt): Constant $C$ effect**\n","\n","Explain the effect that small and large cost constants $C$ should have on the solution. What happens when $C$ goes to infinity?\n","\n","---\n","> **Your explanations here**\n","---"]},{"cell_type":"markdown","metadata":{"id":"WbQdHe0YF5lU"},"source":["### 3.2 Gradient descent **(1.75 pts)**\n","\n","We rewrite the objective function as follows:\n","$$\n","    g(\\mathbf{w},b) = \\frac12 \\|\\mathbf{w}\\|^2 + C \\sum_{\\xi_j>0}(1- y_j[\\mathbf{x}_j \\cdot \\mathbf{w} + b])\n","$$\n","and apply the gradient descent method. The gradient is \n","\\begin{align*}\n","  \\nabla_{\\mathbf{w}} g & = \\text{(mising formula)} \\\\\n","  \\frac{\\partial g}{\\partial b} &= \\text{(missing formula)}\n","\\end{align*}\n","and the method updates the variables, \n","$$\n","  (\\mathbf{w}_{n+1}, b_{n+1})^\\top = (\\mathbf{w}_{n}, b_{n})^\\top - \\alpha \\nabla g(\\mathbf{w}_{n}, b_{n})\n","$$\n","until the stopping criterion is met.\n"]},{"cell_type":"markdown","metadata":{"id":"k_1okxZeFDo8"},"source":["#### **Question 3.2 (0.5 pt): Gradient calculation**\n","\n","Calculate the gradient and plug in the formula below: \n","\n","---\n","> **Your derivations here:**\n","\\begin{align*}\n","  \\nabla_{\\mathbf{w}} g & = \\dots \\\\\n","  \\frac{\\partial g}{\\partial b} &= \\dots\n","\\end{align*}\n","\n","---\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-W896NfUlIfD"},"source":["--- \n","#### **Question 3.3 (1 pt)**  \n","\n","*   Implement the functions below\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RXmJxXVuYRNP"},"outputs":[],"source":["# objective function\n","def g(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: np.float32, C: np.float32) -> np.float32:\n","  # ========= YOUR CODE STARTS HERE ========= #\n","  result = ...\n","  # ========== YOUR CODE ENDS HERE ========== #\n","  return result\n","\n","\n","# gradient calculation\n","def grad_g(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: np.float32, C: np.float32) -> Tuple[np.ndarray, np.float32]:\n","  # ========= YOUR CODE STARTS HERE ========= #\n","  grad_gw = ...\n","  grad_gb = ...\n","  # ========== YOUR CODE ENDS HERE ========== #\n","  return grad_gw, grad_gb\n","\n","\n","# gradient descent\n","def grad_desc_svm(X: np.ndarray, y: np.ndarray, C: np.float32, epsilon: float = 1e-4, n_steps: int = 10000, initial_lr: float = 1e-3) -> Tuple[np.ndarray, np.float32]:\n","  # ========= YOUR CODE STARTS HERE ========= #\n","  # initialization\n","  w = ...\n","  b = ...\n","\n","  # iteration\n","  step = 0\n","  lr = initial_lr\n","  while ...:\n","    step += 1\n","\n","    # decrease lr if needed\n","    lr = ...\n","\n","    # update\n","    w = ...\n","    b = ...\n","\n","  # ========== YOUR CODE ENDS HERE ========== #\n","  return w, b"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"executionInfo":{"elapsed":1260,"status":"ok","timestamp":1679311004040,"user":{"displayName":"Rostyslav Hryniv","userId":"02208197071590517236"},"user_tz":-120},"id":"sguWqm46fuB_","outputId":"330e3a1f-e30c-4c9d-b255-a194437b1b95"},"outputs":[],"source":["# run the gradient descent\n","w, b = grad_desc_svm(X_train, y_train, C=1)\n","\n","# illustrate decision for train and test data \n","plt.figure(figsize=(10,5))\n","\n","plt.subplot(1, 2, 1)\n","idx = np.where(y_train * decision_funct(X_train, w, b) < 0)[0]\n","visualSVM(X_train, y_train, w, b, idx)\n","plt.title(\"Train data\")\n","\n","plt.subplot(1, 2, 2)\n","idx = np.where(y_test * decision_funct(X_test, w, b) < 0)[0]\n","visualSVM(X_test, y_test, w, b, idx)\n","plt.title(\"Test data\")\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Calculate accuracy of the Soft Margin SVM with gradient descent"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":355,"status":"ok","timestamp":1679311034079,"user":{"displayName":"Rostyslav Hryniv","userId":"02208197071590517236"},"user_tz":-120},"id":"P0GmLmcMiU0m","outputId":"fd1fc49a-c552-4c76-ae36-6d30ef960a02"},"outputs":[],"source":["# ========= YOUR CODE STARTS HERE ========= #\n","acc = ...\n","print(\"The accuracy of Soft Margin SVM with gradient descent is\", acc)\n","# ========== YOUR CODE ENDS HERE ========== #"]},{"cell_type":"markdown","metadata":{"id":"Q09m3nWp44go"},"source":["#### **Question 3.4 (0.25 pt): SVM via gradient descent summary**\n","\n","Summarize in a few sequences how the optimal separating hyperplane can be found in non-separated case. Comment on the gradient descent method and its convergence. How does C affect the model?\n","\n","---\n","> **Your explanations here**\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"x078pGRJboXx"},"source":["## 4. Third task: nonlinear SVM **(1.5 pts)**\n","\n","In this part, we'll implement the simplest version of the non-linear SVM, namely the one with the so-called polynomial kernel.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tzdAScqyc-mn"},"source":["### 4.1 The main idea\n","\n","In the case when the data are not linearly separable, we can try a nonlinear classifier function $f(\\mathbf{x})$. For instance, if the red points are above the parabola $x_2 = x_1^2$ and all blue points are below it, then a natural candidate for the classifier is $f(\\mathbf{x}) = -x_1^2 + x_2$.\n","\n","For instance, with every $\\mathbf{x} = (x_1,x_2)^\\top$, we can consider the $6$-tuple \n","$$\n","   \\phi(\\mathbf{x}) = (1, \\sqrt2x_1, \\sqrt2x_2, x_1^2, x_2^2, \\sqrt2x_1x_2)^\\top\n","$$\n","and the classifiying function \n","$$\n","  f(\\mathbf{x}) = \\phi(\\mathbf{x}) \\cdot \\mathbf{w} + b\n","$$\n","with $\\mathbf{w} = (w_0, w_1, \\dots, w_5)^\\top$. \n","\n","#### 4.1.1 Soft margin classifier \n","\n","The soft margin approach with slack variables $\\xi_j = \\max\\{0, 1- y_j \\phi(\\mathbf{x}_j) \\cdot \\mathbf{w}\\}$ leads to the minimization of the objective function \n","$$\n","   g(\\mathbf{w}, \\xi) = \\frac12 \\|\\mathbf{w}\\|^2 + C \\sum_j \\xi_j \n","$$\n","under the constraints that, for each $j$,\n","$$\n","  \\xi_j \\ge 0  \\quad \\text{and} \\quad \n","  y_j [\\phi(\\mathbf{x}_j)\\cdot \\mathbf{w} + b] \\ge 1 - \\xi_j\n","$$\n","\n","#### 4.1.2 Dual Wolfe problem\n","\n","The dual problem leads to almost the same optimization problem as in the linear SVM, with the Lagrange function\n","$$\n","  \\mathscr{L}(\\mathbf{w}, b, \\lambda, \\eta) = \\frac12\\|\\mathbf{w}\\|^2 + C \\sum_j \\xi_j - \\sum_j \\lambda_j  \\bigl(y_j [\\phi(\\mathbf{x}_j) \\cdot \\mathbf{w} + b] - (1 - \\xi_j)\\bigr) - \\sum_j \\eta_j \\xi_j \n","$$\n","Partial derivatives in $\\mathbf{w}$, $b$, and $\\xi$ give\n","$$\n","    \\mathbf{w} = \\sum_j \\lambda_j y_j \\phi(\\mathbf{x}_j),\n","    \\qquad\n","    0 = \\sum_j \\lambda_j y_j, \n","    \\qquad \n","    \\lambda_j = C - \\eta_j \\quad  (\\le C) \n","$$\n","using it in $\\mathscr{L}$, we get the Wolfe dual problem of maximizing the quadratic problem\n","\\begin{equation}\n","   \\mathscr{L}_D = \\sum_j \\lambda_j - \\frac12 \\sum_{j,k} \\lambda_j\\lambda_k y_jy_k \\, \\phi(\\mathbf{x}_j)\\cdot\\phi(\\mathbf{x}_k) \\tag{5}\n","\\end{equation}\n","subject to the restrictions $0 \\le \\lambda_j \\le C$ and $\\sum_j \\lambda_j y_j =0$. \n","\n","#### 4.1.3. The *kernel trick* \n","\n","The *kernel trick* is the observation that the coefficient matrix in (3) can be expressed through \n","$$\n","  K(\\mathbf{x}_j, \\mathbf{x}_k) := \\phi(\\mathbf{x}_j)\\cdot \\phi(\\mathbf{x}_k)  = 1 + 2\\mathbf{x}_j\\cdot\\mathbf{x}_k + (\\mathbf{x}_j\\cdot\\mathbf{x}_k)^2 = \\bigl(1 + \\mathbf{x}_j\\cdot\\mathbf{x}_k\\bigr)^2\n","$$\n","In general, one can consider a polynomial kernel of order $d$:\n","$$\n","  K(\\mathbf{x}_j, \\mathbf{x}_k) = \\bigl(1 + \\mathbf{x}_j\\cdot\\mathbf{x}_k\\bigr)^d\n","$$\n","or of many other forms. In terms of the kernel $K$, the Wolfe dual problem is to maximize \n","$$\n","  \\mathscr{L}_D = \\sum_j \\lambda_j - \\frac12 \\sum_{j,k} \\lambda_j\\lambda_k y_jy_k \\, K(\\mathbf{x}_j, \\mathbf{x}_k)\n","$$\n","subject to $0 \\le \\lambda_j \\le C$. This can be solved e.g. by the SMO method as above.\n"]},{"cell_type":"markdown","metadata":{"id":"2ZH_O4Cn69bp"},"source":["### 4.2 Implementation of quadratic SVM **(1.25 pts)**\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["--- \n","#### **Question 4.1 (0.25 pt)**  \n","\n","* Implement the functions below\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FvWDY9qk7g4V"},"outputs":[],"source":["# the embedding\n","def phi(x: np.ndarray) -> np.ndarray:\n","  # ========= YOUR CODE STARTS HERE ========= #\n","  x_embed = ...\n","  # ========== YOUR CODE ENDS HERE ========== #\n","  return x_embed\n","\n","\n","# embed all datapoints\n","def arr_phi(X: np.ndarray) -> np.ndarray:\n","  # ========= YOUR CODE STARTS HERE ========= #\n","  X_embed = ...\n","  # ========== YOUR CODE ENDS HERE ========== #\n","  return X_embed\n","\n","\n","# the kernel\n","def K(x: np.ndarray, y: np.ndarray) -> np.float32:\n","  # ========= YOUR CODE STARTS HERE ========= #\n","  value = ...\n","  # ========== YOUR CODE ENDS HERE ========== #\n","  return value"]},{"cell_type":"markdown","metadata":{"id":"ec8OwnjhL-nw"},"source":["--- \n","#### **Question 4.2 (0.25 pt)**  \n","\n","*   Implement the function below\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sKSlyf1DyM9A"},"outputs":[],"source":["def partial_kernel_derivative(X: np.ndarray, y:np.ndarray, lmbda: np.ndarray, j: int) -> np.float32:\n","  # ========= YOUR CODE STARTS HERE ========= #\n","  grad = ...\n","  # ========== YOUR CODE ENDS HERE ========== #\n","  return grad"]},{"cell_type":"markdown","metadata":{"id":"HzgkbZcjMMVk"},"source":["--- \n","#### **Question 4.3 (0.5 pt)**  \n","\n","*   Modify the SMO function to solve the new dual Wolfe problem. Pay attention to the new inequality $\\lambda_j \\le C$ that the Lagrange multipliers must satisfy\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GFpzca8DyT1r"},"outputs":[],"source":["def dualSVM_kernel_SMO(X: np.ndarray, y: np.ndarray, C: np.float32, lr: float = 1e-3, n_steps: int = 10000) -> np.ndarray:\n","  lmbda = np.zeros(X.shape[0])\n","  for k in range(n_steps):\n","    # ========= YOUR CODE STARTS HERE ========= #\n","    ...\n","    # ========== YOUR CODE ENDS HERE ========== #\n","\n","  print(\"no. of steps:\", k)\n","  assert len(lmbda[lmbda >= 0]) == len(lmbda)\n","  return lmbda\n","\n","# run the quadratic SVM\n","lmbda = dualSVM_kernel_SMO(X_train, y_train, C=10) "]},{"cell_type":"markdown","metadata":{"id":"kcWHnFV0NZsH"},"source":["--- \n","#### **Question 4.4 (0.25 pt)**  \n","\n","*   Determine the weights $w$ and $b$. For $w$, the formula is explicit; $b$ can be determined similarly to part 2 or in several other ways\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J1sOYva6OfcG"},"outputs":[],"source":["# ========= YOUR CODE STARTS HERE ========= #\n","w = ...\n","b = ...\n","# ========== YOUR CODE ENDS HERE ========== #"]},{"cell_type":"markdown","metadata":{"id":"9DfUai6bOhJi"},"source":["--- \n","#### **Question 4.5 (0.25 pt)**  \n","\n","*   Update the decision function for plotting and visualize the results \n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s6XLFZRi7bLT"},"outputs":[],"source":["def decision_funct(x: np.ndarray, w: np.ndarray, b: np.float32) -> np.ndarray:\n","    # ========= YOUR CODE STARTS HERE ========= #\n","    value = ...\n","    # ========== YOUR CODE ENDS HERE ========== #\n","    return value\n","\n","\n","# illustrate decision for train and test data \n","plt.figure(figsize=(10,5))\n","\n","plt.subplot(1, 2, 1)\n","idx = np.where(y_train * decision_funct(X_train, w, b) < 0)[0]\n","visualSVM(X_train, y_train, w, b, idx)\n","plt.title(\"Train data\")\n","\n","plt.subplot(1, 2, 2)\n","idx = np.where(y_test * decision_funct(X_test, w, b) < 0)[0]\n","visualSVM(X_test, y_test, w, b, idx)\n","plt.title(\"Test data\")\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Calculate accuracy of the Kernel-based classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EWjX-eQQNw9Y","outputId":"20d8904c-8401-42cc-8ac8-a4841c1a7343"},"outputs":[],"source":["# ========= YOUR CODE STARTS HERE ========= #\n","acc = ...\n","print(\"The accuracy of Kernel-based method is\", acc)\n","# ========== YOUR CODE ENDS HERE ========== #"]},{"cell_type":"markdown","metadata":{},"source":["#### **Question 4.6 (up to 1.5 pt): Other Kernels (Additional!)**\n","\n","Find information about other possible kernel functions (for example, RBF kernel). Provide detailed explanation of these kernel functions. Implement some of them, compare with the given polynomial kernel with d=2 in terms of accuracy/efficiency."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ========= YOUR CODE STARTS HERE ========= #\n","\n","# ========== YOUR CODE ENDS HERE ========== #"]},{"cell_type":"markdown","metadata":{"id":"D3V8PyT8N-hy"},"source":["## 5. Standard SVM implementation **(0.25 pt)**\n","\n","Find out functionality of the `sklearn` built-in imlementation of the svm algorithm. Run it on the data to compare with your own implementation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"elapsed":860,"status":"ok","timestamp":1679311340434,"user":{"displayName":"Rostyslav Hryniv","userId":"02208197071590517236"},"user_tz":-120},"id":"lufPj_4M61kI","outputId":"47e6cd7e-7b9e-43fe-895f-e31103c105e9"},"outputs":[],"source":["# ========= YOUR CODE STARTS HERE ========= #\n","clf = ...\n","# ========== YOUR CODE ENDS HERE ========== #\n","clf.fit(X_train, y_train)\n","\n","plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=10, cmap=plt.cm.Paired)\n","\n","# plot the decision function\n","ax = plt.gca()\n","xlim = ax.get_xlim()\n","ylim = ax.get_ylim()\n","\n","# create grid to evaluate model\n","xx = np.linspace(xlim[0], xlim[1], 30)\n","yy = np.linspace(ylim[0], ylim[1], 30)\n","YY, XX = np.meshgrid(yy, xx)\n","xy = np.vstack([XX.ravel(), YY.ravel()]).T\n","Z = clf.decision_function(xy).reshape(XX.shape)\n","\n","# plot decision boundary and margins\n","ax.contour(\n","    XX, YY, Z, colors=\"k\", levels=[-1, 0, 1], alpha=0.5, linestyles=[\"--\", \"-\", \"--\"]\n",")\n","# plot support vectors\n","ax.scatter(\n","    clf.support_vectors_[:, 0],\n","    clf.support_vectors_[:, 1],\n","    s=50,\n","    linewidth=1,\n","    facecolors=\"none\",\n","    edgecolors=\"g\",\n",")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"aiX302iwP-Cf"},"source":["--- \n","#### **Question 5.1 (0.25 pt)**  \n","\n","Find out how to evaluate performance of nonlinear SVM classifier on the test data, and evaluate it"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":252,"status":"ok","timestamp":1679311234277,"user":{"displayName":"Rostyslav Hryniv","userId":"02208197071590517236"},"user_tz":-120},"id":"kDclklqtP8i8","outputId":"5b47c229-1111-4c4a-e7af-4a06230076ac"},"outputs":[],"source":["# Nonlinear SVM classifier evaluation\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import f1_score\n","\n","# ========= YOUR CODE STARTS HERE ========= #\n","acc = ...\n","f1 = ...\n","# ========== YOUR CODE ENDS HERE ========== #\n","print(f\"Accuracy of the model = {acc}, f1-score = {f1}\")"]},{"cell_type":"markdown","metadata":{"id":"4vnJdmCpEW4u"},"source":["## 6. Summary **(0.5 pts)**\n","\n","---\n","\n","\n","\n","Write a short report about the work you did.\n","\n","Do not forget to discuss the following points:\n","\n","*   a general idea of SVM classification;\n","*   difference between hard and soft margin approach;\n","*   SVM as an optimization task\n","*   how Wolfe dual problem allows to optimize implementation and sklearn, and reasons for that difference (if any)\n","*   complexity of the various approaches and algorithms\n","*   any difficulties you had completing the tasks\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Xmpa2qgKL1GY"},"source":["--- \n","#### **Question 6.1 (0.5 pts)**  \n","\n",">   **Your answer comes here**\n","---"]},{"cell_type":"markdown","metadata":{"id":"WaoJSpITQbSO"},"source":["## 7. Comments and suggestions to improve this lab assignment\n"," "]},{"cell_type":"markdown","metadata":{"id":"37ijwjrHUrHF"},"source":["--- \n","#### **Question 7.1**  \n","\n",">   **Your answer comes here**\n","---"]}],"metadata":{"colab":{"provenance":[{"file_id":"1fVjQE2dntoN0adCYGZsassDllLur2Phi","timestamp":1679311917076}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
